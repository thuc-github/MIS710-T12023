{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thuc-github/MIS710-T12023/blob/main/Week%204/MIS710_Lab_4_Logistic_Reg-Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNkJtLWoPpW7"
      },
      "source": [
        "\n",
        "# **MIS710 Lab 4 - Introduction to Logistic Regression**\n",
        "\n",
        "**Author: Associate Professor Lemai Nguyen**\n",
        "\n",
        "Objective:\n",
        "**Breast Cancer Diagnosis**\n",
        "Predict the diagnosis (healthy or cancerous) based on a biopsy dataset.\n",
        "\n",
        "**Context**: The dataset was adapted from a biospy dataset. The dataset contains five (5) biological variables and the target variable. \n",
        "\n",
        "**Data**: \n",
        "V1, V2, V7-V9: biological variables\n",
        "\n",
        "Diagnosis: healthy or cancerous\n",
        "\n",
        "**Source**: adapted from a dataset provided by Dr Mark Griffin, Industry Fellow, University of Queensland; also available at: https://www.kaggle.com/datasets/ukveteran/biopsy-data-on-breast-cancer-patients "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxTV1VTuj9mC"
      },
      "source": [
        "**Loading Libraries and Functions**\n",
        "\n",
        "Read about Logistic Regression at:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "Train Test Split:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split\n",
        "\n",
        "Classification metrics:\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9_TVAw2FOsS"
      },
      "source": [
        "!pip install pydotplus #interface for graph visualisation\n",
        "!pip install graphviz #for graph visualisation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVf5buwkml_I"
      },
      "source": [
        "# load libraries\n",
        "import pandas as pd #for data manipulation and analysis\n",
        "import numpy as np\n",
        " \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn import metrics #Import scikit-learn metrics module for model evaluation\n",
        "from sklearn.metrics import precision_recall_curve, precision_recall_curve, classification_report\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B38Q50gZm1vQ"
      },
      "source": [
        "## **Loading Data**\n",
        "\n",
        "\n",
        "1.   Load the dataset\n",
        "2.   Explore the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8AufWU11ebt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/VanLan0/MIS710/main/biopsy_ln.csv\""
      ],
      "metadata": {
        "id": "6-sHwOqUvDwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXYXIiopFSPX"
      },
      "source": [
        "# load dataset\n",
        "records = pd.read_csv(url)\n",
        "\n",
        "#explore the dataset\n",
        "print(records)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What does the following code do?\n",
        "print(records[50:70])"
      ],
      "metadata": {
        "id": "Rm5pOPRG5hlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records.info()"
      ],
      "metadata": {
        "id": "qaThdphidKgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write your own code to inspect missing data\n",
        "records.isnull().sum()"
      ],
      "metadata": {
        "id": "ytxVLaTRdqaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What does the code below do?\n",
        "print('Sample size:', records.shape[0])\n",
        "print('Number of columns:', records.shape[1]) \n",
        "\n",
        "records.describe()"
      ],
      "metadata": {
        "id": "w1OFeG-20rL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records"
      ],
      "metadata": {
        "id": "UfLELMjdCtrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What does the code below do? Why would you do it?\n",
        "\n",
        "records = records.drop(['ID'], axis=1)\n",
        "records.info()"
      ],
      "metadata": {
        "id": "wRUf0J7ZaFx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Do NOT do if you have done the previous code!\n",
        "ALTERNATIVE way to remove ID: "
      ],
      "metadata": {
        "id": "HFf2FUlvgL3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visually Exploring Data**\n",
        "1. Explore histograms of continuous variables\n",
        "2. Generate barcharts of categorical variables\n",
        "3. Convert data as needed\n",
        "3. Explore relationships among the variables using heatmaps\n",
        "4. Explore logistric regression relationships between variables "
      ],
      "metadata": {
        "id": "RJhziPbjRRem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create histograms\n",
        "for i in records.iloc[:,:]: \n",
        "    plt.hist(records[i])\n",
        "    plt.title(i)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "owNEshpvAb1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create barchats\n",
        "sns.countplot(data=records, x='diagnosis')"
      ],
      "metadata": {
        "id": "VEcOor4L84Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Interpreate the outcome of the following code\n",
        "records['diagnosis'].describe()"
      ],
      "metadata": {
        "id": "LMhWCwXc95r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examine other variables**\n",
        "Run the code and write down your observations"
      ],
      "metadata": {
        "id": "sIvzzfuqgX2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in records.columns[0:4]:\n",
        "    sns.boxplot(data=records, x=i, y='diagnosis')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8HHwvXQLu3gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write your own heatmap using cmap='Blue' and annot=True. Hint: using data=records.corr()\n"
      ],
      "metadata": {
        "id": "EwrpTLYjOIE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can you observe in the heatmap?"
      ],
      "metadata": {
        "id": "IDG1zi7KE_ZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define your own function and call it**"
      ],
      "metadata": {
        "id": "o-q_DuqLExFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert categorical data to numerical \n",
        "def coding_diagnosis(x):\n",
        "    if x=='cancerous': return 1\n",
        "    if x=='healthy': return 0\n",
        "       \n",
        "records['Diagnosis'] = records['diagnosis'].apply(coding_diagnosis)\n",
        "\n",
        "print(records.sample(10))"
      ],
      "metadata": {
        "id": "oeNvz-YvcYpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJz7BvR4yefn"
      },
      "outputs": [],
      "source": [
        "#Another way to convert categorical variables to numerical using LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "records['Diagnosis'] = encoder.fit_transform(records['diagnosis'])\n",
        "print(records.sample(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the above two techniques"
      ],
      "metadata": {
        "id": "wOLDYz1hPrBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting biomarkers and diagnosis uaing a logistric relationship**"
      ],
      "metadata": {
        "id": "UuuKp7YDFP8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.regplot(x=records['V7'], y=records['Diagnosis'], logistic=True, ci=None)"
      ],
      "metadata": {
        "id": "3Quobz1KZEXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZn9TsB0IlVZ"
      },
      "source": [
        "**Feature Selection**\n",
        "\n",
        "Select predictors (attributes) for Classification\n",
        "Set role (Target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "records"
      ],
      "metadata": {
        "id": "MQb0qrIGnaZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eklK57M5Iuba"
      },
      "source": [
        "#Selecting predictors\n",
        "features = ['V1', 'V2', 'V7', 'V8', 'V9'] #you can select a range of columns features = records.columns[0:5]\n",
        "\n",
        "#complete the code below\n",
        "X = records[features]\n",
        "y = records.Diagnosis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZZL19jGNYpv"
      },
      "source": [
        "## **Split the Dataset**\n",
        "\n",
        "Split arrays or matrices into random train and test subsets\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WQVc9ZINezn"
      },
      "source": [
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)  # 80% training and 20% testing \n",
        "\n",
        "#inspect the split datasets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTtxQ1QBNuCM"
      },
      "source": [
        "## **Training a Logistic Regression Model**\n",
        "\n",
        "1.   Train a model using the training dataset\n",
        "2.   Make prediction using the model for the test dataset\n",
        "\n",
        "Read about Logistic Regression Classifier at: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j42G9zCxN7xI"
      },
      "source": [
        "# Create Logitic Regression classifer object\n",
        "\n",
        "#Create an initial Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=100)\n",
        "\n",
        "# Complete the code to train Logistic Regression Classifer with the traning dataset \n",
        "logreg = logreg.fit(X_train, y_train)\n",
        "\n",
        "#Complete the code to make predictions for the test dataset\n",
        "y_pred = logreg.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LytoxXxiOIud"
      },
      "source": [
        "**Inspect Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X3Nu4SCOGnI"
      },
      "source": [
        "#join unseen y_test with predicted value into a data frame\n",
        "inspection=pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})\n",
        "\n",
        "#join X_test with the new dataframe\n",
        "inspection=pd.concat([X_test,inspection], axis=1)\n",
        "\n",
        "inspection.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACm74vWIOM7x"
      },
      "source": [
        "## **Model Evaluation**\n",
        "\n",
        "\n",
        "\n",
        "1.   Calculate Accuracy, Precision, Recall, F1\n",
        "\n",
        "\n",
        "Classification metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y6yLs8SOXBn"
      },
      "source": [
        "#import evaluation functions\n",
        "\n",
        "#Model Evaluation, calculate metrics: Accuracy, Precision, Recall, F1,\n",
        "print(\"Accuracy: \", metrics.accuracy_score(y_test,y_pred))\n",
        "print(\"Precision: \", metrics.precision_score(y_test,y_pred))\n",
        "print(\"Recall: \", metrics.recall_score(y_test,y_pred))\n",
        "print(\"F1: \", metrics.f1_score(y_test,y_pred))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpret the above"
      ],
      "metadata": {
        "id": "852gk72fGX_W"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvYek0KZ-SdW"
      },
      "source": [
        "#print confusion matrix and evaluation report\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgbNM-FkRwcL"
      },
      "source": [
        "**Plot ROC (Receiver operating characteristic) curve and confusion matrix**\n",
        "\n",
        "ROC curve\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html?highlight=plot_roc_curve#sklearn.metrics.plot_roc_curve\n",
        "\n",
        "Confusion matrix\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html?highlight=plot%20confusion%20matrix#sklearn.metrics.plot_confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWReYThjRzgt"
      },
      "source": [
        "#import classes to display RocCurve and Confusion Matrix, read example from the website and try on your own\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "#complete the code to display RocCurve and Confusion Matrix\n",
        "RocCurveDisplay.from_estimator(logreg, X_test, y_test)\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Generate a sequence of points along the x axis\n",
        "x_vals = np.linspace(-10, 10, 100)\n",
        "\n",
        "# Calculate the corresponding y values using the model coefficients\n",
        "coef = logreg.coef_.flatten()\n",
        "intercept = logreg.intercept_\n",
        "y_vals = sigmoid(np.dot(X_test, coef) + intercept)\n",
        "\n",
        "# Plot the sigmoid curve using seaborn\n",
        "sns.lineplot(x=x_vals, y=sigmoid(x_vals), label='Sigmoid Curve')\n",
        "sns.lineplot(x=x_vals, y=sigmoid(np.dot(np.column_stack(([x_vals]*5)), coef) + intercept), label='Model Fit')\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_context('notebook', font_scale=1.2)\n",
        "sns.scatterplot(x=X_test['V7'], y=y_test, color='blue')\n",
        "plt.xlabel('Biomarker e.g. V7')\n",
        "plt.ylabel('Diagnosis')\n",
        "plt.title('Logistic Regression Sigmoid Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6QpSRDB4tbxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(coef[0])"
      ],
      "metadata": {
        "id": "ji0DpFAZvnlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Diagnosis= ', '%.3f' % intercept, '+', '%.3f' %coef[0], '*V1', '+', '%.3f' %coef[1], '*V2', '+', '%.3f' %coef[2], '*V7', '+', '%.3f' %coef[3], '*V8', '+', '%.3f' %coef[4], '*V9')"
      ],
      "metadata": {
        "id": "BWDtaAztv5KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Congratulaitons!**\n",
        "\n",
        "Now do it yourself for other datasets:\n",
        "\n",
        "1.  Habermans survival dataset: https://www.kaggle.com/datasets/gilsousa/ or an adapted dataset: https://raw.githubusercontent.com/VanLan0/MIS710/main/haberman_ln.csv or \n",
        "2.  Tinanic dataset from Lab 1\n",
        "3.  and/or another dataset of your choice\n",
        "\n"
      ],
      "metadata": {
        "id": "QXUMZSTbd7Ik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try it yourself! \n",
        "\n",
        "**Do it yourself:** Repeat the above steps with the telco churn dataset to consilidate your learning\n"
      ],
      "metadata": {
        "id": "DPTXpDZKsCso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries"
      ],
      "metadata": {
        "id": "R90z3fZlnfr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load necessary libraries here\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "from sklearn.metrics import precision_recall_curve, precision_recall_curve, confusion_matrix"
      ],
      "metadata": {
        "id": "RkAnAyf8wpOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dataset"
      ],
      "metadata": {
        "id": "JyiY1CzHxcHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data using pandas.read_csv(filepath_or_url, sep=',')\n",
        "url = 'https://raw.githubusercontent.com/thuc-github/MIS710-T12023/main/Week%204/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "0_ThSWV7ws1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA\n",
        "\n",
        "* How many rows and columns in the dataset? \n",
        "* Return the first n rows.\n",
        "* What are the columns and their datatypes?\n",
        "* Is there any missing values? \n",
        "* Any strong correlation from the dataset?  \n",
        "* How to deal with categorical features? \n",
        "\n"
      ],
      "metadata": {
        "id": "hTe1NMnwx6eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the data types of all the columns\n",
        "df. info()\n",
        "\n",
        "# Converting Total Charges to a numerical data type.\n",
        "df.TotalCharges = pd.to_numeric(df.TotalCharges, errors='coerce')\n",
        "\n",
        "# Check missing values\n",
        "df.isnull().sum()\n",
        "\n",
        "# Handle missing values \n",
        "df.dropna(inplace = True)\n",
        "\n",
        "# Remove customer IDs from the data set\n",
        "df = df.drop(['customerID'], axis = 1)"
      ],
      "metadata": {
        "id": "oBbGE3nzzaSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the predictor variable to binary numeric variables\n",
        "# Convert categorical variables into dummy variables\n",
        "# Get Correlation of \"Churn\" with other variables\n",
        "le = LabelEncoder()\n",
        "\n",
        "df1 = df.copy(deep = True)\n",
        "text_data_features = [i for i in list(df.columns) if i not in list(df.describe().columns)]\n",
        "\n",
        "print('Label Encoder Transformation')\n",
        "for i in text_data_features:\n",
        "    df1[i] = le.fit_transform(df1[i])\n",
        "    print(i,' : ',df1[i].unique(),' = ',le.inverse_transform(df1[i].unique()))\n"
      ],
      "metadata": {
        "id": "C1q9WdWdNWkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration\n",
        "* Demographics (age, gender, partner and dependent status)\n",
        "* Customer account information (Tenures, contracts)\n",
        "* Distribution of services \n",
        "* Relation between variables \n",
        "* Distribution of predictor variable (`Churn`)"
      ],
      "metadata": {
        "id": "FfOo1H8i0QD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "churn = df1[df1['Churn'] == 1].describe().T\n",
        "not_churn = df1[df1['Churn'] == 0].describe().T\n",
        "\n",
        "fig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (5,5))\n",
        "plt.subplot(1,2,1)\n",
        "sns.heatmap(churn[['mean']],annot = True,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f', xticklabels=True, yticklabels=True)\n",
        "plt.title('Churned Customers');\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.heatmap(not_churn[['mean']],annot = True,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f', xticklabels=True, yticklabels=True)\n",
        "plt.title('Not_Churned Customers');\n",
        "\n",
        "fig.tight_layout(pad = 0)"
      ],
      "metadata": {
        "id": "CS6nDw7yNoY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = list(df1['Churn'].value_counts())\n",
        "circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n",
        "\n",
        "fig = plt.subplots(nrows = 1,ncols = 2,figsize = (20,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.pie(circle,labels = ['Not-Churn Customer','Churn Customer'],autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n",
        "       wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n",
        "plt.title('Churn - Not-Churn %');\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "ax = sns.countplot(x='Churn',data = df,edgecolor = 'black')\n",
        "for rect in ax.patches:\n",
        "    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\n",
        "ax.set_xticklabels(['Not-Churn Customers','Churn Customers'])\n",
        "    \n",
        "plt.title('Number of Churn - Not-Churn Customers');\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "92u7RczbOBuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = ['gender','SeniorCitizen','Partner','Dependents'] # Customer Information\n",
        "\n",
        "fig = plt.subplots(nrows = 2,ncols = 2,figsize = (20,14))\n",
        "for i in range(len(l1)):\n",
        "    plt.subplot(2,2,i+1)\n",
        "    ax = sns.countplot(x=l1[i],data = df,hue = \"Churn\",edgecolor = 'black')\n",
        "    for rect in ax.patches:\n",
        "        ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\n",
        "    title = l1[i] + ' vs Churn'\n",
        "    plt.title(title);"
      ],
      "metadata": {
        "id": "lToeOgD7Odxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25, 10))\n",
        "\n",
        "corr = df.apply(lambda x: pd.factorize(x)[0]).corr()\n",
        "\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "ax = sns.heatmap(corr, mask=mask, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, linewidths=.2, cmap='coolwarm', vmin=-1, vmax=1)\n"
      ],
      "metadata": {
        "id": "TXs6NBvxO1k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation \n",
        "\n",
        "\n",
        "1.   Prepare X, y\n",
        "2.   Prepare X_train, X_test, y_train, y_test (hint: using `train_test_split')\n",
        "\n"
      ],
      "metadata": {
        "id": "KmoCcKvKyOHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mms = MinMaxScaler() # Normalization\n",
        "ss = StandardScaler() # Standardization\n",
        "\n",
        "df1['tenure'] = mms.fit_transform(df1[['tenure']])\n",
        "df1['MonthlyCharges'] = mms.fit_transform(df1[['MonthlyCharges']])\n",
        "df1['TotalCharges'] = mms.fit_transform(df1[['TotalCharges']])\n",
        "df1.head()\n"
      ],
      "metadata": {
        "id": "NmsPjXxdQhLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1.drop(columns = ['Churn'])\n",
        "y = df1.Churn.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "_EkCDOn0VS9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model implementation\n",
        "\n",
        "1. Try with the original data. What's the performance?\n",
        "2. Let's add data normalisation. Has the performance been improved?"
      ],
      "metadata": {
        "id": "vtvVRhsy0P29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LogisticRegression().fit(X_train,y_train)\n",
        "y_pred = lr_model.predict(X_test)"
      ],
      "metadata": {
        "id": "JI1wNo1BSbJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance evaluation\n",
        "* Classification report\n",
        "* Confusion matrix \n",
        "* Importance weight\n",
        "* ROC and AUC"
      ],
      "metadata": {
        "id": "zZbv7b2HzPyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "-GVhgXG_Shtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred),\n",
        "                annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
        "    \n",
        "plt.title(\"LOGISTIC REGRESSION CONFUSION MATRIX\",fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RkcZ14rcSx0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_prob = lr_model.predict_proba(X_test)[:,1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "plt.plot(fpr, tpr, label='Logistic Regression')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Logistic Regression ROC Curve',fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6wBDKVknS0vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the weights of all the variables\n",
        "weights = pd.Series(lr_model.coef_[0],\n",
        "                 index=X.columns.values)\n",
        "print (weights.sort_values(ascending = False).plot(kind='bar'))"
      ],
      "metadata": {
        "id": "-XJX4gMkSmLF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}